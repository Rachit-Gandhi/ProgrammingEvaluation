{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeJudge import codejudge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./files/synth-v3/\"\n",
    "problemStatementFilepath = filepath + \"problem_statements_v3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CodeJudge Version: CodeJudge 2025.0.alpha\n"
     ]
    }
   ],
   "source": [
    "judge = codejudge.CodeJudge(promptLocation=\"./CodeJudge/prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "codejudgeAssessments = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Correct Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctSolutionsFilepath = filepath + \"correct_solutions_v3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    with open(problemStatementFilepath+f\"problem{i+1}_statement.txt\", \"r\") as f:\n",
    "        problemStatement = f.read()\n",
    "    with open(correctSolutionsFilepath+f\"problem{i+1}_solution.txt\", \"r\") as f:\n",
    "        correctSolution = f.read()\n",
    "    response = judge.evaluateTaxonomical(problemStatement, correctSolution)\n",
    "    codejudgeAssessments[f\"problem{i+1}_correct\"] = {\"response\": response[\"inconsistencies\"], \"assigned_score\": response[\"score\"], \"expected_score\": 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Incorrect Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectSolutionsFilepath = filepath + \"incorrect_solutions_v3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        with open(problemStatementFilepath+f\"problem{j+1}_statement.txt\", \"r\") as f:\n",
    "            problemStatement = f.read()\n",
    "        with open(incorrectSolutionsFilepath+f\"problem{j+1}-{2*i}pt.txt\", \"r\") as f:\n",
    "            incorrectSolution = f.read()\n",
    "        response = judge.evaluateTaxonomical(problemStatement, incorrectSolution)\n",
    "        codejudgeAssessments[f\"problem{j+1}_{2*i}pt\"] = {\"response\": response[\"inconsistencies\"], \"assigned_score\": response[\"score\"], \"expected_score\": 0.2*i}\n",
    "\n",
    "# for i in range(10):\n",
    "#     with open(problemStatementFilepath+f\"problem{i+1}_statement.txt\", \"r\") as f:\n",
    "#         problemStatement = f.read()\n",
    "#     with open(incorrectSolutionsFilepath+f\"problem{i+1}-2pt.txt\", \"r\") as f:\n",
    "#         incorrectSolution = f.read()\n",
    "#     response = judge.evaluateTaxonomical(problemStatement, incorrectSolution)\n",
    "#     codejudgeAssessments[f\"problem{i+1}_2pt\"] = {\"response\": response[\"inconsistencies\"], \"assigned_score\": response[\"score\"], \"expected_score\": 0.20}\n",
    "\n",
    "# for i in range(10):\n",
    "#     with open(problemStatementFilepath+f\"problem{i+1}_statement.txt\", \"r\") as f:\n",
    "#         problemStatement = f.read()\n",
    "#     with open(incorrectSolutionsFilepath+f\"problem{i+1}-4pt.txt\", \"r\") as f:\n",
    "#         incorrectSolution = f.read()\n",
    "#     response = judge.evaluateTaxonomical(problemStatement, incorrectSolution)\n",
    "#     codejudgeAssessments[f\"problem{i+1}_4pt\"] = {\"response\": response[\"inconsistencies\"], \"assigned_score\": response[\"score\"], \"expected_score\": 0.40}\n",
    "\n",
    "# for i in range(10):\n",
    "#     with open(problemStatementFilepath+f\"problem{i+1}_statement.txt\", \"r\") as f:\n",
    "#         problemStatement = f.read()\n",
    "#     with open(incorrectSolutionsFilepath+f\"problem{i+1}-6pt.txt\", \"r\") as f:\n",
    "#         incorrectSolution = f.read()\n",
    "#     response = judge.evaluateTaxonomical(problemStatement, incorrectSolution)\n",
    "#     codejudgeAssessments[f\"problem{i+1}_6pt\"] = {\"response\": response[\"inconsistencies\"], \"assigned_score\": response[\"score\"], \"expected_score\": 0.60}\n",
    "\n",
    "# for i in range(10):\n",
    "#     with open(problemStatementFilepath+f\"problem{i+1}_statement.txt\", \"r\") as f:\n",
    "#         problemStatement = f.read()\n",
    "#     with open(incorrectSolutionsFilepath+f\"problem{i+1}-8pt.txt\", \"r\") as f:\n",
    "#         incorrectSolution = f.read()\n",
    "#     response = judge.evaluateTaxonomical(problemStatement, incorrectSolution)\n",
    "#     codejudgeAssessments[f\"problem{i+1}_8pt\"] = {\"response\": response[\"inconsistencies\"], \"assigned_score\": response[\"score\"], \"expected_score\": 0.80}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"codejudge_results_synth-v3.json\", \"w\") as f:\n",
    "    json.dump(codejudgeAssessments, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedAssessments = json.load(open(\"codejudge_results_synth-v3.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"codejudge_results_synth-v3.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Problem\", \"Assigned Score\", \"Expected Score\"])\n",
    "    for key in loadedAssessments:\n",
    "        problem = key\n",
    "        writer.writerow([problem, loadedAssessments[key][\"assigned_score\"], loadedAssessments[key][\"expected_score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
